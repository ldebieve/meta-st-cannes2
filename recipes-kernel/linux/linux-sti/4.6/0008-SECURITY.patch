From 66e3eaf98fe07bba60cc118119f2ad04b343cd9c Mon Sep 17 00:00:00 2001
From: Christophe Priouzeau <christophe.priouzeau@st.com>
Date: Fri, 22 Jul 2016 14:50:34 +0200
Subject: [PATCH 08/12] SECURITY

Signed-off-by: Christophe Priouzeau <christophe.priouzeau@st.com>
---
 arch/arm/include/asm/outercache.h |  9 ++++
 arch/arm/include/asm/tz.h         | 94 +++++++++++++++++++++++++++++++++++++++
 arch/arm/mach-sti/board-dt.c      |  9 ++++
 arch/arm/mm/cache-l2x0.c          | 92 +++++++++++++++++++++++++++++---------
 4 files changed, 184 insertions(+), 20 deletions(-)
 create mode 100644 arch/arm/include/asm/tz.h

diff --git a/arch/arm/include/asm/outercache.h b/arch/arm/include/asm/outercache.h
index c2bf24f..a739245 100644
--- a/arch/arm/include/asm/outercache.h
+++ b/arch/arm/include/asm/outercache.h
@@ -39,6 +39,8 @@ struct outer_cache_fns {
 	/* This is an ARM L2C thing */
 	void (*write_sec)(unsigned long, unsigned);
 	void (*configure)(const struct l2x0_regs *);
+
+	bool (*tz_mutex)(unsigned long *);
 };
 
 extern struct outer_cache_fns outer_cache;
@@ -129,4 +131,11 @@ static inline void outer_resume(void) { }
 
 #endif
 
+#define outer_tz_mutex outer_tz_mutex
+static inline bool outer_tz_mutex(unsigned long *lock)
+{
+	if (outer_cache.tz_mutex)
+		return outer_cache.tz_mutex(lock);
+	return false;
+}
 #endif	/* __ASM_OUTERCACHE_H */
diff --git a/arch/arm/include/asm/tz.h b/arch/arm/include/asm/tz.h
new file mode 100644
index 0000000..bfd3231
--- /dev/null
+++ b/arch/arm/include/asm/tz.h
@@ -0,0 +1,94 @@
+#ifndef __ASM_ARM_TZ_H
+#define __ASM_ARM_TZ_H
+
+/* Support of ARMv7 TrustZone generic features */
+#if (__LINUX_ARM_ARCH__ < 7) || !defined(CONFIG_SMP)
+
+/* no ATM TZ or no SMP => no need for shared spinlocks */
+static inline void tz_spin_lock(unsigned long *lock) { }
+static inline int tz_spin_trylock(unsigned long *lock) { return 1; }
+static inline void tz_spin_unlock(unsigned long *lock) { }
+
+#else
+
+#include <asm/processor.h>
+
+/*
+ * Shared spinning mutex support
+ *
+ * Shared mutex between linux and TrustZone worlds require use of very basic
+ * ARMv6+ DDR mutex cells:
+ * - lock is defined by the value stored in a 32bit DDR cell (4 byte aligned).
+ * - value is 0: mutex is not locked, value is 1: mutex is locked.
+ * - use of ldrex/strex instructions and a memory barrier when required.
+ * - basic power saving: WFE while lock is locked, SEV on lock release.
+ * - no extra complexity.
+ *
+ * Actually, this is the pre kernel 3.6 ARM arch_spinlock support.
+ */
+static inline void tz_spin_lock(unsigned long *lock)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+"1:	ldrex	%0, [%1]\n"
+"	teq	%0, #0\n"
+	WFE("ne")
+"	strexeq	%0, %2, [%1]\n"
+"	teqeq	%0, #0\n"
+"	bne	1b"
+	: "=&r" (tmp)
+	: "r" (lock), "r" (1)
+	: "cc");
+/*
+ * ARMv6+ ticket-based spin-locking.
+ *
+ * A memory barrier is required after we get a lock, and before we
+ * release it, because V6+ CPUs are assumed to have weakly ordered
+ * memory.
+ */
+	smp_mb();
+}
+
+static inline int tz_spin_trylock(unsigned long *lock)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+"	ldrex	%0, [%1]\n"
+"	teq	%0, #0\n"
+"	strexeq	%0, %2, [%1]"
+	: "=&r" (tmp)
+	: "r" (lock), "r" (1)
+	: "cc");
+
+	if (tmp)
+		return 0;
+/*
+ * ARMv6+ ticket-based spin-locking.
+ *
+ * A memory barrier is required after we get a lock, and before we
+ * release it, because V6+ CPUs are assumed to have weakly ordered
+ * memory.
+ */
+	smp_mb();
+	return 1;
+}
+
+static inline void tz_spin_unlock(unsigned long *lock)
+{
+/*
+ * ARMv6+ ticket-based spin-locking.
+ *
+ * A memory barrier is required after we get a lock, and before we
+ * release it, because V6+ CPUs are assumed to have weakly ordered
+ * memory.
+ */
+	smp_mb();
+	*lock = 0;
+
+	dsb_sev();
+}
+#endif
+
+#endif /* __ASM_ARM_TZ_H */
diff --git a/arch/arm/mach-sti/board-dt.c b/arch/arm/mach-sti/board-dt.c
index ae10fb2..893a158 100644
--- a/arch/arm/mach-sti/board-dt.c
+++ b/arch/arm/mach-sti/board-dt.c
@@ -14,6 +14,14 @@
 
 #include "smp.h"
 
+static void sti_l2_write_sec(unsigned long val, unsigned reg)
+{
+	/*
+	 * We can't write to secure registers as we are in non-secure
+	 * mode, until we have some SMI service available.
+	 */
+}
+
 static const char *const stih41x_dt_match[] __initconst = {
 	"st,stih415",
 	"st,stih416",
@@ -31,4 +39,5 @@ DT_MACHINE_START(STM, "STiH415/416 SoC with Flattened Device Tree")
 			  L2C_AUX_CTRL_WAY_SIZE(4),
 	.l2c_aux_mask	= 0xc0000fff,
 	.smp		= smp_ops(sti_smp_ops),
+	.l2c_write_sec	= sti_l2_write_sec,
 MACHINE_END
diff --git a/arch/arm/mm/cache-l2x0.c b/arch/arm/mm/cache-l2x0.c
index 9f9d542..308cf9b 100644
--- a/arch/arm/mm/cache-l2x0.c
+++ b/arch/arm/mm/cache-l2x0.c
@@ -31,6 +31,7 @@
 #include <asm/cputype.h>
 #include <asm/hardware/cache-l2x0.h>
 #include "cache-tauros3.h"
+#include <asm/tz.h>
 #include "cache-aurora-l2.h"
 
 struct l2c_init_data {
@@ -82,6 +83,26 @@ static void l2c_write_sec(unsigned long val, void __iomem *base, unsigned reg)
 }
 
 /*
+ * Shared mutex to synchronise L2CC maintenance between linux
+ * world and secure world (ARM TZ).
+ */
+static unsigned long *l2x0_tz_mutex;
+
+#define l2x0_spin_lock_irqsave(_flags) \
+	do {							\
+		raw_spin_lock_irqsave(&l2x0_lock, _flags);	\
+		if (l2x0_tz_mutex != NULL)				\
+			tz_spin_lock(l2x0_tz_mutex);		\
+	} while (0)
+
+#define l2x0_spin_unlock_irqrestore(_flags) \
+	do {							\
+		if (l2x0_tz_mutex != NULL)				\
+			tz_spin_unlock(l2x0_tz_mutex);		\
+		raw_spin_unlock_irqrestore(&l2x0_lock, _flags);	\
+	} while (0)
+
+/*
  * This should only be called when we have a requirement that the
  * register be written due to a work-around, as platforms running
  * in non-secure mode may not be able to access this register.
@@ -241,6 +262,28 @@ static void l2c210_sync(void)
 	__l2c210_cache_sync(l2x0_base);
 }
 
+/* Enable/disable external mutex shared with ARM TZ */
+static bool l2x0_tz_mutex_cfg(unsigned long *lock)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&l2x0_lock, flags);
+
+	if (lock && l2x0_tz_mutex && (lock != l2x0_tz_mutex)) {
+		raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+		pr_err("%s: a TZ mutex is already enabled\n", __func__);
+		return false;
+	}
+
+	l2x0_tz_mutex = lock;
+	/* Ensure mutex pointer is updated before lock is released */
+	smp_wmb();
+
+	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	pr_info("%s: %sable TZ mutex\n\n", __func__, (lock) ? "en" : "dis");
+	return true;
+}
+
 static const struct l2c_init_data l2c210_data __initconst = {
 	.type = "L2C-210",
 	.way_size_0 = SZ_8K,
@@ -257,6 +300,7 @@ static const struct l2c_init_data l2c210_data __initconst = {
 		.disable = l2c_disable,
 		.sync = l2c210_sync,
 		.resume = l2c_resume,
+		.tz_mutex = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -280,10 +324,10 @@ static void l2c220_op_way(void __iomem *base, unsigned reg)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c_op_way(base + reg);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static unsigned long l2c220_op_pa_range(void __iomem *reg, unsigned long start,
@@ -314,7 +358,7 @@ static void l2c220_inv_range(unsigned long start, unsigned long end)
 	void __iomem *base = l2x0_base;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	if ((start | end) & (CACHE_LINE_SIZE - 1)) {
 		if (start & (CACHE_LINE_SIZE - 1)) {
 			start &= ~(CACHE_LINE_SIZE - 1);
@@ -333,7 +377,7 @@ static void l2c220_inv_range(unsigned long start, unsigned long end)
 				   start, end, flags);
 	l2c_wait_mask(base + L2X0_INV_LINE_PA, 1);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_clean_range(unsigned long start, unsigned long end)
@@ -347,12 +391,12 @@ static void l2c220_clean_range(unsigned long start, unsigned long end)
 		return;
 	}
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	flags = l2c220_op_pa_range(base + L2X0_CLEAN_LINE_PA,
 				   start, end, flags);
 	l2c_wait_mask(base + L2X0_CLEAN_INV_LINE_PA, 1);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_flush_range(unsigned long start, unsigned long end)
@@ -366,12 +410,12 @@ static void l2c220_flush_range(unsigned long start, unsigned long end)
 		return;
 	}
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	flags = l2c220_op_pa_range(base + L2X0_CLEAN_INV_LINE_PA,
 				   start, end, flags);
 	l2c_wait_mask(base + L2X0_CLEAN_INV_LINE_PA, 1);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_flush_all(void)
@@ -383,9 +427,9 @@ static void l2c220_sync(void)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c220_cache_sync(l2x0_base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_enable(void __iomem *base, unsigned num_lock)
@@ -422,6 +466,7 @@ static const struct l2c_init_data l2c220_data = {
 		.disable = l2c_disable,
 		.sync = l2c220_sync,
 		.resume = l2c_resume,
+		.tz_mutex = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -477,7 +522,7 @@ static void l2c310_inv_range_erratum(unsigned long start, unsigned long end)
 		unsigned long flags;
 
 		/* Erratum 588369 for both clean+invalidate operations */
-		raw_spin_lock_irqsave(&l2x0_lock, flags);
+		l2x0_spin_lock_irqsave(flags);
 		l2c_set_debug(base, 0x03);
 
 		if (start & (CACHE_LINE_SIZE - 1)) {
@@ -494,7 +539,7 @@ static void l2c310_inv_range_erratum(unsigned long start, unsigned long end)
 		}
 
 		l2c_set_debug(base, 0x00);
-		raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+		l2x0_spin_unlock_irqrestore(flags);
 	}
 
 	__l2c210_op_pa_range(base + L2X0_INV_LINE_PA, start, end);
@@ -533,12 +578,12 @@ static void l2c310_flush_all_erratum(void)
 	void __iomem *base = l2x0_base;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	l2c_set_debug(base, 0x03);
 	__l2c_op_way(base + L2X0_CLEAN_INV_WAY);
 	l2c_set_debug(base, 0x00);
 	__l2c210_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void __init l2c310_save(void __iomem *base)
@@ -786,6 +831,7 @@ static const struct l2c_init_data l2c310_init_fns __initconst = {
 		.disable = l2c310_disable,
 		.sync = l2c210_sync,
 		.resume = l2c310_resume,
+		.tz_mutex = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1110,6 +1156,7 @@ static const struct l2c_init_data of_l2c210_data __initconst = {
 		.disable     = l2c_disable,
 		.sync        = l2c210_sync,
 		.resume      = l2c_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1130,6 +1177,7 @@ static const struct l2c_init_data of_l2c220_data __initconst = {
 		.disable     = l2c_disable,
 		.sync        = l2c220_sync,
 		.resume      = l2c_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1291,6 +1339,7 @@ static const struct l2c_init_data of_l2c310_data __initconst = {
 		.disable     = l2c310_disable,
 		.sync        = l2c210_sync,
 		.resume      = l2c310_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1320,6 +1369,7 @@ static const struct l2c_init_data of_l2c310_coherent_data __initconst = {
 		.flush_all   = l2c210_flush_all,
 		.disable     = l2c310_disable,
 		.resume      = l2c310_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1366,10 +1416,10 @@ static void aurora_pa_range(unsigned long start, unsigned long end,
 	while (start < end) {
 		range_end = aurora_range_end(start, end);
 
-		raw_spin_lock_irqsave(&l2x0_lock, flags);
+		l2x0_spin_lock_irqsave(flags);
 		writel_relaxed(start, base + AURORA_RANGE_BASE_ADDR_REG);
 		writel_relaxed(range_end - CACHE_LINE_SIZE, base + offset);
-		raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+		l2x0_spin_unlock_irqrestore(flags);
 
 		writel_relaxed(0, base + AURORA_SYNC_REG);
 		start = range_end;
@@ -1404,9 +1454,9 @@ static void aurora_flush_all(void)
 	unsigned long flags;
 
 	/* clean all ways */
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c_op_way(base + L2X0_CLEAN_INV_WAY);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 
 	writel_relaxed(0, base + AURORA_SYNC_REG);
 }
@@ -1421,12 +1471,12 @@ static void aurora_disable(void)
 	void __iomem *base = l2x0_base;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c_op_way(base + L2X0_CLEAN_INV_WAY);
 	writel_relaxed(0, base + AURORA_SYNC_REG);
 	l2c_write_sec(0, base, L2X0_CTRL);
 	dsb(st);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void aurora_save(void __iomem *base)
@@ -1499,6 +1549,7 @@ static const struct l2c_init_data of_aurora_with_outer_data __initconst = {
 		.disable     = aurora_disable,
 		.sync	     = aurora_cache_sync,
 		.resume      = l2c_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1671,6 +1722,7 @@ static const struct l2c_init_data of_bcm_l2x0_data __initconst = {
 		.disable     = l2c310_disable,
 		.sync        = l2c210_sync,
 		.resume      = l2c310_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
-- 
1.9.1

